---
layout: post
title: "ArcFace"
date: 2022-07-02 12:00:00 -0500
image: cv.jpg
tags: [ai, metriclearning, arcface]
categories: [ai]
---

# ARCFACE METRIC LEARNING INTRODUCE

# Metric이란?
Distance(거리) = (distance=metirc)은 함수이다.   
단 다음의 조건을 만족하는 함수여야 한다.
1. D(x,y) = 0 (x=y): 자기 자신과의 거리는 0이다
2. D(x,y) = D(y,x): 거리는 상호 대칭성을 가진다
3. D(x,y) <= D(x,z) + D(z,y): 거리는 삼각 부등식을 만족한다

### 자주 쓰이는 metric 함수 종류
1. 유클리디안 거리 = 2차 노름 거리 = $\sqrt {\sum_{i=1}^{n}(x_i - y_i)^2}$
2. P차 노름거리 = ${p}\sqrt {\sum_{i=1}^{n}(x_i - y_i)^2}$
3. 내적 거리 = $x \,• \, y$ = $\sum_{i=1}^{n} {x_i} \,• \,{y_i}$
4. 코사인 거리 = 1 - $\frac {x*y}{||x||||y||}$
5. 마할라노비스 거리 = $\sqrt {(x-y)^T\sum{}^{-1}(x-y)} $   $\sum$
은 데이터 공분산행렬

Metric space는 두 개체 사이의 거리가 정의 된 공간 
=  유사항 개체는 가까이, 유사하지 않은 개체는 멀리 위치한 공간

# Deep metric learning이란?
딥 뉴럴 네트워크로 거리 공간을 학습한다.

유사도는 의사결정에 유용하게 사용된다.   
유사도가 활용된 대표 알고리즘  
비지도 학습 : K-means(군집) | 지도학습: kNN(수치,범주예측)

process: 
1. feature engineering을 한 후 latent/embedding space가 된다.

딥러닝 모델로 거리공간을 학습(Metrig learning)을 통해 가능.

# Loss function
## softmax의 한계  
In Face recognition
1. face identification: 데이터베이스 내 인물 중 누구와 가장 유사한지  식별(일대다 유사성 계산)
2. Face cerification: 동일인 여부 검증, 일대일 유사성 계산

1. Softmax classifier로 형성되는 embedding space특징
Seperable features: 적당히 구분 가능한 특징이 학습 된다(학습에 사용되는 category의 수가 많기 때문에)
but, 정확한 얼굴 인식을 위해 필요한 embedding space는 Discriminative features이다 (inter-class variation은 크고, inter -clas variation은 작게)

2. Openset의 특징 추출 효과적x, 새로운 인물에 대한 판단이 어려움

# Deep metric learing 손실 함수 종류
### ▷  Contrastive loss (2005 CVPR)  
: 가장 간단하며 직관적인 손실함수, 두개의 이미지 사용

$$L_{contrastive}(\theta,X_1,X_2,Y_{similar})$$
= $$Y_{similar}(D^2(f_\theta(X_1),f_\theta(x_2))+(1-Y_{similar})max(0,m-D^2(f_\theta(X_1),f_\theta(X_2)))$$

- $(X_1,X_2)$: 이미지1,2
- $y_{similar}$: 두개가 이미지가 유사한지 아닌지 정보 제공 
- $Y_{similar}(D^2(f_\theta(X_1),f_\theta(x_2))$: 두 이미지가 같으면, 거리값을 줄임
- $(1-Y_{similar})max(0,m-D^2(f_\theta(X_1),f_\theta(X_2)))$: 두이미지가 유사하지 않은  데이터라면 멀어지도록 
- m = margin(하이퍼파라미터)
- if $m<D^2(f_\theta(X_1),t_\theta(X_2))$ then $m-D^2(f_\theta(X_1),f_\theta(X_2))<0$
- 마진보다 거리가 크면 m-거리는 0보다 작게되고, 0보다 비교해서 가장 큰값이 0이기 때문에, m보다 둘 사이의 거리가 크면 거리를 더 멀게 조정하지 않음


### ▷ Triplet loss (2015 CVPR)
: 가장 유명하고 널리 활용되는 손실 함수
세 개의 데이터(anchor, positive, negative)사용하고, anchro와 positive의 pair은 anchor와 negative pair보다 작도록ㅡ 심지어 postive에 margine이 더해져도 더 적도록!! anchor와 positive는 가깝게, negative는 멀게
$$ D(f_\theta(X_a),f_\theta(X_p))+m < D(f_\theta(X_a),f_\theta(X_n) $$
== $$D(f_\theta(X_a),f_\theta(X_p))+m - D(f_\theta(X_a),f_\theta(X_n) <0 $$
== $$L_{triplet}(\theta,X_1,X_2) = max(0,D^2(f_\theta(X_a),f_\theta(X_p)) - D^2(f_\theta(X_a),f_\theta(X_n)+m $$ 

- $f_\theta(X_p)$: Positive embedding
- $f_\theta(X_a)$: Anchor embedding
- $f_\theta(X_n)$: Negative embedding
- m: margin


---- 
여기까지는 class에 속하는지(spervised learning metric learning이였다.)
이후는 label을 활용 하지 않은 방식 (self-supervised learning 자가지도 학습 활용)즉 같은 이미지와 유사하면1  아니면 0

#### Triplet loss의 단점
1. mining이 성능에 영향을 많이 줌 (세미나28분)
2. 어떤 negative인지에 따라 성능/계산량 차이가 크게남
3. 대규모 데이터 셋 (클래스 다량)을 학습 할 경우 이러한 triplet 쌍을 뽑을 수 있는 경 우가 기하 급수적으로 많아져 학습이 까다로움.

### ▷ Center loss(2016 ECCV)
Triplet loss의 단점을 개선하기 위해 등장  
class 간거리는 멀게, class내 거리는 좁게를 직접적으로 손실함수에 반영

$$L_{center} = L_{softmax} + \frac{\lambda}{2}\sum_{i=1}^{N}||f_\theta(X_i) - C_{yi}||_{2}^{2}$$

- $f_\theta(X_i)$: DNN에서 나온 embedding value 
- k: class의 수

- $L_{softmax} =(W^Tf_\theta(X_i)+b$):  최종적인 클래스의 수를 나누는 softmax를 학습하기 위해서는 vector의 크기 조정이 필요해서 Linear layer가 붙어 다음과 같이 나온다 클래스가 거리간 멀어지는 학습을 진행.
- $\frac{\lambda}{2}\sum_{i=1}^{N}||f_\theta(X_i) - C_{yi}||_{2}^{2}$: class내 variation을 줄이는 학습을 진행한다.  입력된 X가 가진$(X_i)$ y레이블을 대표하는 vector를 $C_{yi}$라고 하면, 대표벡터와 이미지 간의 거리를 줄이는 식이고 이를 softmax에 붙인다.


### ▷ Additive angular margin loss (2019 CVPR)
Arcface에 loss로 나옴
softmax함수를 변형한 새로운 해석 제시 (내적)을 사용하여 각도로 구분될  수 있는 손실함수 
b=0이라고 하면 $W^T$의 i번째 행과 임베딩벡터 $f_\theta(X_i)$의 내적 유사도로 결정이 된다고 봄 (유도과정 생략..)

$$-\frac{1}{N}\sum_{i=1}^{N}log\frac{exp{(s\,• \,cos(\theta_{yi}+m)})}{exp(s \,• \, cos(\theta_{y_i^{,i}}+m)+\sum_{j=1,j\ne{y_i}}^{K})exp(s\,• \,cos(\theta_{j,i}))}$$

- $W_{yi}$: 정답인 클래스의 벡터

- $W_{j\ne{y_i}}$: 정답이 아닌 클래스의 벡터

- $f_\theta(X_i)$: 들어온 이미지의 벡터 

- $\theta_{y_i^{,i}}$: 같은 클래스의 각도 값

- $\theta_{y_j,i}$: 정답이 아닌 것과 클래스의 각도 값

따라서 $\theta_{y_i}^{,i} +m < \theta_{j,i}$가 되도록 한다 







```
reference  DMQA Seminar | 
```


# ▷ FaceNet (by Google, CVPR 2015)
• 구글에있는얼굴이미지약2억6천만장학습.  
• 단일모델  
• 메트릭러닝 + Triplet loss  
• Labeled Faces in the Wild (LFW) Data (약 13,000 이미지, 1680 인물, 인물별 이미지  
최대 14장, 최소 1장)에서 정확도 99.63%  
• 얼굴 이미지 임베딩을 이용하여 얼굴인식(Face identification), 얼굴인증 (Faceverification) 가능

# ▷ ArcFace (2019)
FaceNet 과 구조는 같지만 loss function에 변형을 주어 발전 된 모델 연구((SphereFace, CosFace, ArcFace etc.)    
가장 최근에 나온 ArcFace (2019)는 현재 까지 나온 얼굴인식    알고리즘에 서 가장 좋은 정확도 보여줄 뿐만 아니 라 얼굴 이미지 말고 다른 데이터에서 도 좋은 정확도 보임.  

Triplet loss에서 Anchor, Postivie, Negative sample 간의 거리를 최적화하는 반면, 하나의 sample과 sample 에 해당하는 클래스의 중심간의 거리와 다른 identity의 중심간의 거리를 최적화한다.